# -*- coding: utf-8 -*-
"""enhancing_rag_with_graph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZUsv5hew2t_I-qROBF19KvK-a-NEv5AV
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  langchain langchain-community langchain-ollama langchain-experimental neo4j tiktoken yfiles_jupyter_graphs python-dotenv json-repair langchain-openai langchain_core

"""Installing packages:
-langchain
-langchain-community
-langchain-ollama
-langchain-experimental
-neo4j
-tiktoken
-yfiles_jupyter_graphs
-python-dotenv
-json-repair
-langchain-openai
-langchain_core
"""

from langchain_core.runnables import  RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser
from langchain_community.graphs import Neo4jGraph
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_models import ChatOllama
from langchain_experimental.graph_transformers import LLMGraphTransformer
from neo4j import GraphDatabase
from yfiles_jupyter_graphs import GraphWidget
from langchain_community.vectorstores import Neo4jVector
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars
from langchain_ollama import OllamaEmbeddings
import os
from langchain_experimental.llms.ollama_functions import OllamaFunctions
from neo4j import  Driver

from dotenv import load_dotenv

#load_dotenv("env.env")
load_dotenv("env.env")

"""Importing the Required Libraries"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %pip install pdfplumber pytesseract

"""This installs the libraries that are required for extracting text, tables, and images from PDF files. This also provides a better standard PDF parser.
-pdfplumber- used for text extraction in digital PDF's
-pytesseract- used for extracting text from scanned PDF's which may be photos or handwritten notes

-This can eventally help us with converting PDF's into structured text before storing them in the Neo4j

"""

import pdfplumber
import pandas as pd
from PIL import Image
import pytesseract
import io
import numpy as np
from typing import Dict, List, Union, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PDFParser:
    def __init__(self, pdf_path: str, header_height: float = 0.1, footer_height: float = 0.1):
        """
        Initialize the PDF parser

        Args:
            pdf_path (str): Path to the PDF file
            header_height (float): Percentage of page height to ignore at top (default: 0.1 or 10%)
            footer_height (float): Percentage of page height to ignore at bottom (default: 0.1 or 10%)
        """
        self.pdf_path = pdf_path
        self.header_height = header_height
        self.footer_height = footer_height
        self.pdf = None

    def is_handwritten(self, page) -> bool:
        """
        Check if a page contains handwritten content that would require OCR
        This is a basic implementation that can be enhanced based on specific needs

        Args:
            page: PDF page object

        Returns:
            bool: True if handwritten content is detected
        """
        # Convert page to image
        img = page.to_image()
        img_bytes = io.BytesIO()
        img.save(img_bytes, format='PNG')
        image = Image.open(img_bytes)

        # Convert to grayscale and analyze pixel distribution
        gray_image = image.convert('L')
        pixels = np.array(gray_image)

        # Calculate standard deviation of pixel values
        # Handwritten content typically has higher variance in pixel values
        std_dev = np.std(pixels)

        # This threshold can be adjusted based on your specific needs
        return std_dev > 75  # Arbitrary threshold, adjust based on testing

    def extract_content(self) -> Dict[str, Union[List[str], List[pd.DataFrame]]]:
        """
        Extract content from the PDF while ignoring headers and footers

        Returns:
            Dict containing lists of extracted text and tables
        """
        try:
            with pdfplumber.open(self.pdf_path) as pdf:
                self.pdf = pdf
                all_text = []
                all_tables = []

                for page in pdf.pages:
                    # Check for handwritten content
                    if self.is_handwritten(page):
                        raise ValueError("Handwritten content detected. OCR processing required.")

                    # Get page dimensions
                    height = page.height
                    header_boundary = height * self.header_height
                    footer_boundary = height * (1 - self.footer_height)

                    # Extract content from the middle section of the page
                    middle_section = {
                        'top': header_boundary,
                        'bottom': footer_boundary,
                        'left': 0,
                        'right': page.width
                    }

                    # Extract tables first
                    tables = page.within_bbox(
                        (middle_section['left'], middle_section['top'],
                         middle_section['right'], middle_section['bottom'])
                    ).extract_tables()

                    if tables:
                        for table in tables:
                            # Convert table to pandas DataFrame and clean it
                            df = pd.DataFrame(table[1:], columns=table[0])
                            df = df.replace('', pd.NA).dropna(how='all')
                            all_tables.append(df)

                    # Extract text excluding table areas
                    text = page.within_bbox(
                        (middle_section['left'], middle_section['top'],
                         middle_section['right'], middle_section['bottom'])
                    ).extract_text()

                    if text.strip():
                        all_text.append(text.strip())

                return {
                    'text': all_text,
                    'tables': all_tables
                }

        except Exception as e:
            logger.error(f"Error processing PDF: {str(e)}")
            raise

    def get_formatted_text(self) -> str:
        """
        Get well-formatted text content from the PDF

        Returns:
            str: Formatted text content
        """
        content = self.extract_content()
        return '\n\n'.join(content['text'])

    def get_tables(self) -> List[pd.DataFrame]:
        """
        Get list of tables from the PDF

        Returns:
            List[pd.DataFrame]: List of pandas DataFrames containing table data
        """
        content = self.extract_content()
        return content['tables']

    def save_tables_to_excel(self, output_path: str):
        """
        Save extracted tables to an Excel file

        Args:
            output_path (str): Path to save the Excel file
        """
        tables = self.get_tables()
        if not tables:
            logger.warning("No tables found in the PDF")
            return

        with pd.ExcelWriter(output_path) as writer:
            for i, df in enumerate(tables):
                df.to_excel(writer, sheet_name=f'Table_{i+1}', index=False)

        logger.info(f"Tables saved to {output_path}")



"""This is the PDFParser calss. Its main function is to extract text and tables from the PDF. One thing that must be done is ignoring header and footers. it also includes handwrtitten notes scanning and saving tables to Excel sheets. Extracting from the PDF's is done using pdfplumber.
To accomodate for headers and footers, only parts of the page is excluding from the scanning. Converts extracted tables into Pandas Data Frames. And saves extracted tables to an Excel file.

def init: takes a PDF file path. Specifies how muhc of the top and bottom to ignore.
"""

import logging
import os

def test_pdf_parsing(textbook):
      # Initialize parser with default header/footer settings
      parser = PDFParser(textbook)

      # Create output directory if it doesn't exist
      output_dir = "parsed_output"
      os.makedirs(output_dir, exist_ok=True)

      # Get text content (text_content pass to the graph)
      text_content = parser.get_formatted_text()

      #return the text
      return text_content

#textbook being passed for extraction
#textbook = "/content/test book.pdf"
textbook = "test_book.pdf"
text_content = test_pdf_parsing(textbook)

"""Chatgpt will generate the query for us, eliminating the need to format the entities and relationships in json. However chat doesn't retain memory of previously created data, I suspect that it generates duplicate nodes as a result"""

import os
import openai

def chat_generate_cypher(text):
    client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    prompt = f"""
    You are a helpful assistant. Generate a valid Cypher query for a Neo4j database
    from the following academic textbook text. Extract key entities and
    relationships to create nodes and relationships. Output only the raw Cypher query. Make sure to have the metadata in each node based on the given text.



    Note: Do not include any explanations or apologies in your responses.
    Text: {text}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            temperature=0.0,
        )
        cypher_query = response.choices[0].message.content.strip()
        return cypher_query
    except Exception as e:
        print(f"Error generating Cypher query: {e}")
        return None

def remove_code_block_markers(text, language='cypher'):
    lines = text.strip().split('\n')
    if lines and lines[0].startswith(f'```{language}'):
        lines = lines[1:]
    else:
        logging.warning("The input does not start with the expected code block marker.")
    if lines and lines[-1].strip() == '```':
        lines = lines[:-1]
    else:
        logging.warning("The input does not end with the expected code block marker.")
    cleaned_text = '\n'.join(lines)
    return cleaned_text


from neo4j import GraphDatabase

class Neo4jHandler:
    def __init__(self, uri, user, password):
        self.driver = GraphDatabase.driver(uri, auth=(user, password))

    def close(self):
        self.driver.close()

    def execute_cypher(self, cypher_query):
        with self.driver.session() as session:
            result = session.run(cypher_query)
            return result

# Initialize your Neo4j connection (replace credentials as needed)
neo4j_handler = Neo4jHandler("neo4j+s://7e941b1b.databases.neo4j.io", "neo4j","wRRRL9KfUsdtpd_ws1kGLLxC8zb3L_RvV_bsmsyntU8")

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Assuming text_content is your academic textbook content
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
refined_chunks = text_splitter.split_text(text_content)

batch_size = 10
total_chunks = len(refined_chunks)
all_graph_documents = []

for i in range(0, total_chunks, batch_size):
    batch = refined_chunks[i:i + batch_size]
    graph_documents = []

    for chunk in batch:
        # Generate a Cypher query from the text chunk
        cypher_query = chat_generate_cypher(chunk)
        cypher_query = remove_code_block_markers(cypher_query)
        # Debugging: print the generated Cypher query
        print(f"Generated Cypher Query:\n{cypher_query}\n")

        if cypher_query:
            try:
                # Execute the generated query in Neo4j
                neo4j_handler.execute_cypher(cypher_query)

                # Optionally, store the query (or confirmation) as a Document
                graph_documents.append(Document(page_content=cypher_query, metadata={"source": "generated_cypher"}))
            except Exception as e:
                print(f"Error executing Cypher query: {e}")

    all_graph_documents.extend(graph_documents)
    batch_num = i // batch_size + 1
    print(f"Processed batch {batch_num} with {len(graph_documents)} chunks.")

neo4j_handler.close()



"""This is the working graph code (which is the same code as the oldest version):"""

def showGraph():
    driver = GraphDatabase.driver(
        uri=os.environ["NEO4J_URI"],
        auth=(os.environ["NEO4J_USERNAME"],
        os.environ["NEO4J_PASSWORD"]))
    session = driver.session()
    widget = GraphWidget(graph = session.run("MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t").graph())
    widget.node_label_mapping = 'id'
    return widget
showGraph()

"""Support for third party widgets will remain active for the duration of the session. To disable support:"""

from google.colab import output
output.disable_custom_widget_manager()

from google.colab import output
output.enable_custom_widget_manager()